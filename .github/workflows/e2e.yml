name: E2E Tests
on:
  workflow_dispatch:

jobs:
  e2e-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Cache Docker build layers
      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ hashFiles('backend/Dockerfile', 'frontend/Dockerfile') }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      # Cache Hugging Face Models (aligned with docker-compose mount path)
      - name: Cache Hugging Face Models
        uses: actions/cache@v4
        with:
          path: data/cache
          key: hf-models-${{ hashFiles('backend/requirements.txt') }}
          restore-keys: |
            hf-models-

      # Build images with cache
      - name: Build Backend Image
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          file: ./backend/Dockerfile
          target: production
          push: false
          load: true
          tags: vellum-backend:latest
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max

      - name: Build Frontend Image
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          target: production
          build-args: |
            VITE_API_URL=/api/v1
            VITE_BYPASS_AUTH=true
          push: false
          load: true
          tags: vellum-frontend:latest
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max

      - name: Build E2E Test Runner
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          target: test
          push: false
          load: true
          tags: vellum-e2e:latest
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max

      # Create override for CI (use OpenAI instead of Ollama)
      - name: Create CI Override
        run: |
          cat > docker-compose.ci.yml << 'EOF'
          services:
            backend:
              environment:
                - OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}
                # Override to use OpenAI model instead of Ollama
                - OLLAMA_BASE_URL=
              depends_on:
                # Remove Ollama dependency in CI
                ollama:
                  condition: service_started
                  required: false
          EOF

      - name: Run E2E Tests
        env:
          CI: true
        run: |
          docker compose -f docker-compose.test.yml -f docker-compose.ci.yml up --abort-on-container-exit --exit-code-from e2e

      # Move cache to prevent unbounded growth
      - name: Move cache
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache